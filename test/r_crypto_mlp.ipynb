{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    seed = 42069  # set a random seed for reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight(random=False):\n",
    "    if random:\n",
    "        rng = default_rng(0)\n",
    "         # y0 dim: (1, 2)\n",
    "        w1 = rng.standard_normal((3, 2), dtype='f')\n",
    "        # y1 dim: (1, 3)\n",
    "        w2 = rng.standard_normal((3, 3), dtype='f')\n",
    "        # y2 dim: (1, 3)\n",
    "        w3 = rng.standard_normal((2, 3), dtype='f')\n",
    "    else:\n",
    "        # y0 dim: (1, 2)\n",
    "        w1 = np.array([[0.2, 0.3],\n",
    "                       [0.4, 0.2],\n",
    "                       [0.3, 0.4]], dtype='f')\n",
    "        # y1 dim: (1, 3)\n",
    "        w2 = np.array([[0.2, 0.3, 0.4],\n",
    "                       [0.4, 0.2, 0.3],\n",
    "                       [0.3, 0.4, 0.2]], dtype='f')\n",
    "        # y2 dim: (1, 3)\n",
    "        w3 = np.array([[0.2, 0.3, 0.4],\n",
    "                       [0.4, 0.2, 0.3]], dtype='f')\n",
    "        # y3 dim: (1, 2)\n",
    "    return w1, w2, w3\n",
    "\n",
    "\n",
    "class CMlp(nn.Module):\n",
    "\n",
    "    def __init__(self, encrypt=False, weight_random=False):\n",
    "        super(CMlp, self).__init__()\n",
    "        w1, w2, w3 = init_weight(random=weight_random)\n",
    "        self.fc1 = nn.Linear(2, 3, False)\n",
    "        self.fc2 = nn.Linear(3, 3, False)\n",
    "        self.fc3 = nn.Linear(3, 2, False)\n",
    "\n",
    "        if encrypt:\n",
    "            noise_rng = default_rng(1)\n",
    "            self.r1 = noise_rng.standard_normal((3, 1), dtype='f')\n",
    "            self.r2 = noise_rng.standard_normal((3, 1), dtype='f')\n",
    "            # self.r3 = noise_rng.standard_normal((2, 1), dtype='f')\n",
    "            self.r3 = noise_rng.standard_normal((1, 1), dtype='f')\n",
    "\n",
    "            self.fc1.weight.data = torch.from_numpy(w1 * self.r1)\n",
    "            self.fc2.weight.data = torch.from_numpy(w2 * self.r2 / self.r1.transpose())\n",
    "            self.fc3.weight.data = torch.from_numpy(w3 * self.r3 / self.r2.transpose())\n",
    "        else:\n",
    "            self.fc1.weight.data = torch.from_numpy(w1)\n",
    "            self.fc2.weight.data = torch.from_numpy(w2)\n",
    "            self.fc3.weight.data = torch.from_numpy(w3)\n",
    "\n",
    "        self.y2 = None\n",
    "        self.y3 = None\n",
    "        self.alpha = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.fc1(x)\n",
    "        self.y2 = self.fc2(y1)\n",
    "        self.alpha = self.y2.sum()\n",
    "        self.y3 = self.fc3(self.y2)\n",
    "        self.y3.retain_grad()\n",
    "        return self.y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- plaintext weight ---------------\n",
      "tensor([[ 1.1176, -1.3871],\n",
      "        [-0.4266, -0.8036],\n",
      "        [ 0.6014, -0.0750]], device='cuda:0')\n",
      "tensor([[ 0.0597, -0.0320, -0.1855],\n",
      "        [ 1.2048,  0.7775, -1.3583],\n",
      "        [ 0.7698, -0.8702,  1.0998]], device='cuda:0')\n",
      "tensor([[-0.9585, -1.2749, -1.3653],\n",
      "        [-1.4743,  0.4335, -0.3281]], device='cuda:0')\n",
      "y:  tensor([[ 0.4749, -0.3197]], device='cuda:0', grad_fn=<MmBackward>)\n",
      "----------- plaintext grad -----------------\n",
      "tensor([[-0.0165, -0.0248],\n",
      "        [-0.1109, -0.1664],\n",
      "        [ 0.1088,  0.1632]], device='cuda:0')\n",
      "tensor([[-0.2374, -0.4023,  0.1205],\n",
      "        [ 0.0623,  0.1056, -0.0316],\n",
      "        [-0.0584, -0.0990,  0.0297]], device='cuda:0')\n",
      "tensor([[ 4.8124e-04,  1.5515e-02, -6.1016e-03],\n",
      "        [ 1.5730e-02,  5.0711e-01, -1.9944e-01]], device='cuda:0')\n",
      "----------- ciphertext weight ---------------\n",
      "tensor([[ 1.9325, -2.3985],\n",
      "        [ 0.6093,  1.1479],\n",
      "        [ 0.6181, -0.0771]], device='cuda:0')\n",
      "tensor([[ 0.0650,  0.0422, -0.3397],\n",
      "        [ 0.0524, -0.0409, -0.0993],\n",
      "        [ 0.1858,  0.2542,  0.4465]], device='cuda:0')\n",
      "tensor([[  0.9269,  30.8851,   5.9564],\n",
      "        [  1.4258, -10.5022,   1.4315]], device='cuda:0')\n",
      "----------- ciphertext grad ---------------\n",
      "tensor([[-0.7627, -1.1441],\n",
      "        [-0.0652, -0.0977],\n",
      "        [ 0.2168,  0.3252]], device='cuda:0')\n",
      "tensor([[  0.3823,  -0.5352,  -0.1154],\n",
      "        [ 14.3215, -20.0486,  -4.3217],\n",
      "        [  2.6677,  -3.7344,  -0.8050]], device='cuda:0')\n",
      "tensor([[ 0.0493,  0.0634, -0.1385],\n",
      "        [-0.0030, -0.0038,  0.0083]], device='cuda:0')\n",
      "Get yc:  tensor([[-0.8644,  0.5820]], device='cuda:0', grad_fn=<MmBackward>)\n",
      "Get yc from y:  tensor([[-0.8644,  0.5820]], device='cuda:0', grad_fn=<MulBackward0>)\n",
      "Ly derivative\n",
      "tensor([[-0.0251, -0.8197]], device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor([[-0.0251, -0.8197]], device='cuda:0')\n",
      "Lhaty derivative\n",
      "tensor([[-1.3644,  0.0820]], device='cuda:0', grad_fn=<SubBackward0>)\n",
      "tensor([[-1.3644,  0.0820]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# setup gpu or cpu\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "x = torch.tensor([[0.2, 0.3]], device=device)\n",
    "y_hat = torch.tensor([[0.5, 0.5]], device=device)\n",
    "\n",
    "net = CMlp(weight_random=True).to(device)\n",
    "print('----------- plaintext weight ---------------')\n",
    "for p in net.parameters():\n",
    "    print(p.data)\n",
    "y = net(x)\n",
    "print('y: ', y)\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(y, y_hat)\n",
    "loss.backward(retain_graph=True)\n",
    "print('----------- plaintext grad -----------------')\n",
    "for p in net.parameters():\n",
    "    print(p.grad)\n",
    "w_gradlist = [p.grad for p in net.parameters()]\n",
    "print('----------- ciphertext weight ---------------')\n",
    "net_c = CMlp(encrypt=True, weight_random=True).to(device)\n",
    "for p in net_c.parameters():\n",
    "    print(p.data)\n",
    "y_c = net_c(x)\n",
    "c_loss = criterion(y_c, y_hat)\n",
    "c_loss.backward(retain_graph=True)\n",
    "print('----------- ciphertext grad ---------------')\n",
    "for p in net_c.parameters():\n",
    "    print(p.grad)\n",
    "c_w_gradlist = [p.grad.detach().clone() for p in net_c.parameters()]\n",
    "\n",
    "print('Get yc: ', y_c)\n",
    "r3 = torch.from_numpy(net_c.r3.transpose()).to(device)\n",
    "print('Get yc from y: ', y  * r3)\n",
    "print('Ly derivative')\n",
    "print(y - y_hat)\n",
    "print(y.grad)\n",
    "y_grad = y.grad.clone()\n",
    "print('Lhaty derivative')\n",
    "print(y_c - y_hat)\n",
    "print(y_c.grad)\n",
    "y_c_grad = y_c.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$ \\frac{\\partial \\widehat{L}}{\\partial \\widehat{W}^{(l)}} = ( \\frac{\\partial L}{\\partial y^{(L)}} + (r^L - 1) \\cdot y^{L}) \\frac{\\partial \\widehat{y}^{(L)}}{\\partial \\widehat{W}^{(l)}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Optimizer(net.parameters(), {})\n",
    "optim_c = torch.optim.Optimizer(net_c.parameters(), {})\n",
    "optim_c.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3393,  0.9017]], device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n3 = r3.clone()\n",
    "r_L = (n3 - 1) * y\n",
    "print(r_L)\n",
    "t = y_grad + r_L\n",
    "y_c.backward(t, retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### get grad $\\frac{\\partial \\widehat{L}}{\\partial \\widehat{W}^{(l)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7627, -1.1441],\n",
      "        [-0.0652, -0.0977],\n",
      "        [ 0.2168,  0.3252]], device='cuda:0')\n",
      "tensor([[  0.3823,  -0.5352,  -0.1154],\n",
      "        [ 14.3215, -20.0486,  -4.3217],\n",
      "        [  2.6677,  -3.7344,  -0.8050]], device='cuda:0')\n",
      "tensor([[ 0.0493,  0.0634, -0.1385],\n",
      "        [-0.0030, -0.0038,  0.0083]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for p in net_c.parameters():\n",
    "    print(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7627, -1.1441],\n",
      "        [-0.0652, -0.0977],\n",
      "        [ 0.2168,  0.3252]], device='cuda:0')\n",
      "tensor([[  0.3823,  -0.5352,  -0.1154],\n",
      "        [ 14.3215, -20.0486,  -4.3217],\n",
      "        [  2.6677,  -3.7344,  -0.8050]], device='cuda:0')\n",
      "tensor([[ 0.0493,  0.0634, -0.1385],\n",
      "        [-0.0030, -0.0038,  0.0083]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(*c_w_gradlist, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\frac{\\partial \\widehat{L}}{\\partial \\widehat{W}^{(l)}} = \\frac{r^L}{R_l} \\frac{\\partial L}{\\partial {W}^{(l)}}+ (r^L - 1) \\cdot y^{L} \\frac{\\partial \\widehat{y}^{(L)}}{\\partial \\widehat{W}^{(l)}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.57833433]\n",
      " [-0.7000578 ]\n",
      " [ 0.9730042 ]]\n",
      "[[  0.9187248   -0.75898033   0.54607177]\n",
      " [ 23.013092   -19.011662    13.678524  ]\n",
      " [  4.1443925   -3.4237807    2.4633443 ]]\n",
      "[[-1.0340182  -0.0412799  -0.22922012]]\n",
      "tensor([[ 0.0174,  0.0261],\n",
      "        [-0.1414, -0.2121],\n",
      "        [-0.1927, -0.2891]], device='cuda:0')\n",
      "tensor([[ 0.3970, -0.5558, -0.1198],\n",
      "        [-2.6093,  3.6527,  0.7874],\n",
      "        [ 0.4406, -0.6167, -0.1329]], device='cuda:0')\n",
      "tensor([[ 0.0009,  0.0012, -0.0025],\n",
      "        [ 0.0296,  0.0381, -0.0832]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "denoise = [1 / net_c.r1, net_c.r1.transpose() / net_c.r2, net_c.r2.transpose() / net_c.r3]\n",
    "print(*denoise, sep='\\n') # noise for w_ij in each layer\n",
    "\n",
    "# left of addition\n",
    "l_tensor_list =[]\n",
    "for x, n in zip(w_gradlist, denoise):\n",
    "    print(x.mul(torch.from_numpy(n).to(device)).mul(torch.from_numpy(net_c.r3).to(device)))\n",
    "    l_tensor_list.append(x.mul(torch.from_numpy(n).to(device)).mul(torch.from_numpy(net_c.r3).to(device)))\n",
    "\n",
    "optim_c.zero_grad()\n",
    "y_c.backward(r_L, retain_graph=True)\n",
    "\n",
    "# right of addition\n",
    "r_tensor_list = [p.grad.detach().clone() for p in net_c.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### restore ciphertext grad in left of the equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7627, -1.1441],\n",
      "        [-0.0652, -0.0977],\n",
      "        [ 0.2168,  0.3252]], device='cuda:0')\n",
      "tensor([[  0.3823,  -0.5352,  -0.1154],\n",
      "        [ 14.3215, -20.0487,  -4.3217],\n",
      "        [  2.6677,  -3.7344,  -0.8050]], device='cuda:0')\n",
      "tensor([[ 0.0493,  0.0634, -0.1385],\n",
      "        [-0.0030, -0.0038,  0.0083]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for l, r in zip(l_tensor_list, r_tensor_list):\n",
    "    print(l + r)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "$ \\frac{R_l}{r^L} (\\frac{\\partial \\widehat{L}}{\\partial \\widehat{W}^{(l)}} - (r^L - 1) \\cdot y^{L} \\frac{\\partial \\widehat{y}^{(L)}}{\\partial \\widehat{W}^{(l)}})  =  \\frac{\\partial L}{\\partial {W}^{(l)}} $"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0165, -0.0248],\n",
      "        [-0.1109, -0.1664],\n",
      "        [ 0.1088,  0.1632]], device='cuda:0')\n",
      "tensor([[-0.2374, -0.4023,  0.1205],\n",
      "        [ 0.0623,  0.1056, -0.0316],\n",
      "        [-0.0584, -0.0990,  0.0297]], device='cuda:0')\n",
      "tensor([[ 4.8124e-04,  1.5514e-02, -6.1016e-03],\n",
      "        [ 1.5730e-02,  5.0711e-01, -1.9944e-01]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "denoise_cipher = [net_c.r1, net_c.r2 / net_c.r1.transpose(), net_c.r3 / net_c.r2.transpose()]\n",
    "\n",
    "for x, y, n in zip(c_w_gradlist, r_tensor_list, denoise_cipher):\n",
    "    z = x - y\n",
    "    print(z.mul(torch.from_numpy(n).to(device)).mul(torch.from_numpy(1 / net_c.r3).to(device)))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}