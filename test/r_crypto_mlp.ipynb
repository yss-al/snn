{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    seed = 42069  # set a random seed for reproducibility\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight(random=False):\n",
    "    if random:\n",
    "        rng = default_rng(0)\n",
    "         # y0 dim: (1, 2)\n",
    "        w1 = rng.standard_normal((3, 2), dtype='f')\n",
    "        # y1 dim: (1, 3)\n",
    "        w2 = rng.standard_normal((3, 3), dtype='f')\n",
    "        # y2 dim: (1, 3)\n",
    "        w3 = rng.standard_normal((2, 3), dtype='f')\n",
    "    else:\n",
    "        # y0 dim: (1, 2)\n",
    "        w1 = np.array([[0.2, 0.3],\n",
    "                       [0.4, 0.2],\n",
    "                       [0.3, 0.4]], dtype='f')\n",
    "        # y1 dim: (1, 3)\n",
    "        w2 = np.array([[0.2, 0.3, 0.4],\n",
    "                       [0.4, 0.2, 0.3],\n",
    "                       [0.3, 0.4, 0.2]], dtype='f')\n",
    "        # y2 dim: (1, 3)\n",
    "        w3 = np.array([[0.2, 0.3, 0.4],\n",
    "                       [0.4, 0.2, 0.3]], dtype='f')\n",
    "        # y3 dim: (1, 2)\n",
    "    return w1, w2, w3\n",
    "\n",
    "\n",
    "class CMlp(nn.Module):\n",
    "\n",
    "    def __init__(self, encrypt=False, weight_random=False):\n",
    "        super(CMlp, self).__init__()\n",
    "        w1, w2, w3 = init_weight(random=weight_random)\n",
    "        self.fc1 = nn.Linear(2, 3, False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(3, 3, False)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(3, 2, False)\n",
    "\n",
    "        if encrypt:\n",
    "            noise_rng = default_rng(1234)\n",
    "            self.r1 = np.absolute(noise_rng.standard_normal((3, 1), dtype='f'))\n",
    "            self.r2 = np.absolute(noise_rng.standard_normal((3, 1), dtype='f'))\n",
    "            self.r3 = np.absolute(noise_rng.standard_normal((2, 1), dtype='f'))\n",
    "            self.fc1.weight.data = torch.from_numpy(w1 * self.r1)\n",
    "            self.fc2.weight.data = torch.from_numpy(w2 * self.r2 / self.r1.transpose())\n",
    "            self.fc3.weight.data = torch.from_numpy(w3 * self.r3 / self.r2.transpose())\n",
    "        else:\n",
    "            self.fc1.weight.data = torch.from_numpy(w1)\n",
    "            self.fc2.weight.data = torch.from_numpy(w2)\n",
    "            self.fc3.weight.data = torch.from_numpy(w3)\n",
    "\n",
    "        self.y2 = None\n",
    "        self.y3 = None\n",
    "        self.alpha = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = self.fc1(x)\n",
    "        self.y2 = self.fc2(self.relu1(y1))\n",
    "        self.alpha = self.y2.sum()\n",
    "        self.y3 = self.fc3(self.relu2(self.y2))\n",
    "        self.y3.retain_grad()\n",
    "        return self.y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------- plaintext weight ---------------\n",
      "tensor([[ 1.1176, -1.3871],\n",
      "        [-0.4266, -0.8036],\n",
      "        [ 0.6014, -0.0750]])\n",
      "tensor([[ 0.0597, -0.0320, -0.1855],\n",
      "        [ 1.2048,  0.7775, -1.3583],\n",
      "        [ 0.7698, -0.8702,  1.0998]])\n",
      "tensor([[-0.9585, -1.2749, -1.3653],\n",
      "        [-1.4743,  0.4335, -0.3281]])\n",
      "y:  tensor([[-0.1468, -0.0353]], grad_fn=<MmBackward>)\n",
      "----------- plaintext grad -----------------\n",
      "tensor([[0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.2329, 0.3493]])\n",
      "tensor([[0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1035]])\n",
      "tensor([[-0.0000, -0.0000, -0.0696],\n",
      "        [-0.0000, -0.0000, -0.0576]])\n",
      "----------- ciphertext weight ---------------\n",
      "tensor([[ 2.1577, -2.6780],\n",
      "        [-1.1628, -2.1905],\n",
      "        [ 0.9163, -0.1143]])\n",
      "tensor([[ 0.0345, -0.0131, -0.1358],\n",
      "        [ 0.2053,  0.0938, -0.2933],\n",
      "        [ 0.5553, -0.4446,  1.0052]])\n",
      "tensor([[-0.1470, -0.6633, -0.1678],\n",
      "        [-0.4237,  0.4225, -0.0755]])\n",
      "----------- ciphertext grad ---------------\n",
      "tensor([[0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0255, 0.0382]])\n",
      "tensor([[0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0189]])\n",
      "tensor([[-0.0000, -0.0000, -0.0786],\n",
      "        [-0.0000, -0.0000, -0.0766]])\n",
      "Get yc:  tensor([[-0.0251, -0.0113]], grad_fn=<MmBackward>)\n",
      "Get yc from y:  tensor([[-0.0251, -0.0113]], grad_fn=<MulBackward0>)\n",
      "Ly derivative\n",
      "tensor([[-0.6468, -0.5353]], grad_fn=<SubBackward0>)\n",
      "tensor([[-0.6468, -0.5353]])\n",
      "Lhaty derivative\n",
      "tensor([[-0.5251, -0.5113]], grad_fn=<SubBackward0>)\n",
      "tensor([[-0.5251, -0.5113]])\n"
     ]
    }
   ],
   "source": [
    "# setup gpu or cpu\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "\n",
    "x = torch.tensor([[0.2, 0.3]], device=device)\n",
    "y_hat = torch.tensor([[0.5, 0.5]], device=device)\n",
    "\n",
    "net = CMlp(weight_random=True).to(device)\n",
    "print('----------- plaintext weight ---------------')\n",
    "for p in net.parameters():\n",
    "    print(p.data)\n",
    "y = net(x)\n",
    "print('y: ', y)\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(y, y_hat)\n",
    "loss.backward(retain_graph=True)\n",
    "print('----------- plaintext grad -----------------')\n",
    "for p in net.parameters():\n",
    "    print(p.grad)\n",
    "w_gradlist = [p.grad for p in net.parameters()]\n",
    "print('----------- ciphertext weight ---------------')\n",
    "net_c = CMlp(encrypt=True, weight_random=True).to(device)\n",
    "for p in net_c.parameters():\n",
    "    print(p.data)\n",
    "y_c = net_c(x)\n",
    "c_loss = criterion(y_c, y_hat)\n",
    "c_loss.backward(retain_graph=True)\n",
    "print('----------- ciphertext grad ---------------')\n",
    "for p in net_c.parameters():\n",
    "    print(p.grad)\n",
    "c_w_gradlist = [p.grad.detach().clone() for p in net_c.parameters()]\n",
    "\n",
    "print('Get yc: ', y_c)\n",
    "r3 = torch.from_numpy(net_c.r3.transpose()).to(device)\n",
    "print('Get yc from y: ', y  * r3)\n",
    "print('Ly derivative')\n",
    "print(y - y_hat)\n",
    "print(y.grad)\n",
    "y_grad = y.grad.clone()\n",
    "print('Lhaty derivative')\n",
    "print(y_c - y_hat)\n",
    "print(y_c.grad)\n",
    "y_c_grad = y_c.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "$ \\frac{\\partial \\widehat{L}}{\\partial \\widehat{W}^{(l)}} = ( \\frac{\\partial L}{\\partial y^{(L)}} + (r^L - 1) \\cdot y^{L}) \\frac{\\partial \\widehat{y}^{(L)}}{\\partial \\widehat{W}^{(l)}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Optimizer(net.parameters(), {})\n",
    "optim_c = torch.optim.Optimizer(net_c.parameters(), {})\n",
    "optim_c.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1711, 0.3206]])\n"
     ]
    }
   ],
   "source": [
    "r_L = r3.clone()\n",
    "r_L_1y = (r_L - 1) * y\n",
    "print(r_L)\n",
    "t = y_grad + r_L_1y\n",
    "y_c.backward(t, retain_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### get grad $\\frac{\\partial \\widehat{L}}{\\partial \\widehat{W}^{(l)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0255, 0.0382]])\n",
      "tensor([[0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0189]])\n",
      "tensor([[ 0.0000,  0.0000, -0.0786],\n",
      "        [ 0.0000,  0.0000, -0.0766]])\n",
      "Assert cipher grad\n",
      "tensor([[0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.0255, 0.0382]])\n",
      "tensor([[0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0189]])\n",
      "tensor([[-0.0000, -0.0000, -0.0786],\n",
      "        [-0.0000, -0.0000, -0.0766]])\n"
     ]
    }
   ],
   "source": [
    "for p in net_c.parameters():\n",
    "    print(p.grad)\n",
    "\n",
    "print('Assert cipher grad') \n",
    "print(*c_w_gradlist, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equation\n",
    "\n",
    "$ \\frac{\\partial \\widehat{L}}{\\partial \\widehat{W}^{(l)}} = \\frac{r^L}{R_l} \\frac{\\partial L}{\\partial {W}^{(l)}}+ (r^L - 1) \\cdot y^{L} \\frac{\\partial \\widehat{y}^{(L)}}{\\partial \\widehat{W}^{(l)}} $\n",
    "\n",
    "$ R_l \\cdot (\\frac{1}{r^L} \\cdot \\frac{\\partial \\widehat{L}}{\\partial \\widehat{y}^{(L)}} \\cdot \\frac{\\partial \\widehat{y}^{(L)}}{\\partial \\widehat{W}^{(l)}} - \\frac{1}{r^L} \\cdot (r^L - 1) \\cdot y^{L} \\frac{\\partial \\widehat{y}^{(L)}}{\\partial \\widehat{W}^{(l)}}) = \\frac{\\partial L}{\\partial {W}^{(l)}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.51797825]\n",
      " [0.3668524 ]\n",
      " [0.65637   ]]\n",
      "[[1.7304813 2.443358  1.3656197]\n",
      " [5.8691945 8.287025  4.6317096]\n",
      " [1.3863815 1.9575051 1.0940711]]\n",
      "[[6.5187154 1.921987  8.136661 ]\n",
      " [3.4799373 1.0260295 4.3436575]]\n"
     ]
    }
   ],
   "source": [
    "# Denoise vars\n",
    "denoise = [1 / net_c.r1, net_c.r1.transpose() / net_c.r2, net_c.r2.transpose() / net_c.r3]\n",
    "print(*denoise, sep='\\n') # noise for w_ij in each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute left of equation\n",
    "\n",
    "lvar: $ \\frac{1}{r^L} \\cdot \\frac{\\partial \\widehat{L}}{\\partial \\widehat{y}^{(L)}} $, and backward\n",
    "\n",
    "rvar: $ \\frac{1}{r^L} \\cdot (r^L - 1) \\cdot y^{L} $, and backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lvar: tensor([[-3.0684, -1.5949]])\n",
      "tensor([[0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.1277, 0.1916]])\n",
      "tensor([[0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0947]])\n",
      "tensor([[ 0.0000,  0.0000, -0.4595],\n",
      "        [ 0.0000,  0.0000, -0.2388]])\n"
     ]
    }
   ],
   "source": [
    "frac_r_L = 1 / r_L\n",
    "lvar = frac_r_L * y_c_grad\n",
    "print(f'lvar: {lvar}')\n",
    "optim_c.zero_grad()\n",
    "y_c.backward(lvar, retain_graph=True)\n",
    "left_result_list = [p.grad.clone() for p in net_c.parameters()]\n",
    "print(*left_result_list, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rvar: tensor([[0.7111, 0.0748]], grad_fn=<MulBackward0>)\n",
      "tensor([[ 0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000],\n",
      "        [-0.0251, -0.0377]])\n",
      "tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000, -0.0186]])\n",
      "tensor([[0.0000, 0.0000, 0.1065],\n",
      "        [0.0000, 0.0000, 0.0112]])\n"
     ]
    }
   ],
   "source": [
    "rvar = frac_r_L * r_L_1y\n",
    "print(f'rvar: {rvar}')\n",
    "optim_c.zero_grad()\n",
    "y_c.backward(rvar, retain_graph=True)\n",
    "right_result_list = [p.grad.clone() for p in net_c.parameters()]\n",
    "print(*right_result_list, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get plaintext grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0000],\n",
      "        [0.0000, 0.0000],\n",
      "        [0.2329, 0.3493]])\n",
      "tensor([[0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.1035]])\n",
      "tensor([[ 0.0000,  0.0000, -0.0696],\n",
      "        [ 0.0000,  0.0000, -0.0576]])\n"
     ]
    }
   ],
   "source": [
    "for term1, term2, den in zip(left_result_list, right_result_list, denoise):\n",
    "    print((term1 - term2).div(torch.from_numpy(den).to(device)))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e47ec2281ab428d4ca22f0f74965e4cc9a389189477cc90a3919c72a06fb43f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('torch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}